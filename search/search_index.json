{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Duy M. Nguyen's notebook","text":"<p>M\u1ee5c \u0111\u00edch c\u1ee7a trang blog n\u00e0y l\u00e0 \u0111\u1ec3 c\u00e1 nh\u00e2n m\u00ecnh note l\u1ea1i m\u1ecdi th\u1ee9 m\u00e0 m\u00ecnh th\u1ea5y c\u1ea7n thi\u1ebft cho b\u1ea3n th\u00e2n, bao g\u1ed3m nh\u1eefng th\u1ee9 li\u00ean quan \u0111\u1ebfn c\u00f4ng vi\u1ec7c engineer c\u1ee7a m\u00ecnh, t\u1eeb m\u1ea5y c\u00e1i nh\u1ecf nh\u01b0 c\u00e1c thu\u1eadt to\u00e1n thi\u1ebfu nhi, \u0111\u1ebfn m\u1ea5y c\u00e1i l\u1edbn h\u01a1n nh\u01b0 System Design, hay c\u00e1c thu\u1eadt to\u00e1n Machine Learning,... (ch\u1eafc ch\u1ee7 y\u1ebfu s\u1ebd l\u00e0 nh\u1eefng c\u00e1i nh\u1ecf, v\u00ec m\u1ea5y c\u00e1i to m\u00ecnh l\u01b0\u1eddi vi\u1ebft )</p> <p>Ngo\u00e0i ra nh\u1eefng th\u1ee9 linh tinh nh\u01b0 c\u1ea3m nh\u1eadn c\u00e1 nh\u00e2n sau khi xem m\u1ed9t b\u1ed9 phim, review v\u1ec1 m\u1ed9t m\u00f3n \u0111\u1ed3 linh tinh n\u00e0o \u0111\u1ea5y mua tr\u00ean s\u00e0n S hay s\u00e0n L, hay c\u1ea3m x\u00fac sau m\u1ed9t chuy\u1ebfn \u0111i ch\u01a1i \u1edf \u0111\u00e2u \u0111\u1ea5y th\u00ec c\u00f3 th\u1ec3 c\u0169ng \u0111\u01b0\u1ee3c ghi l\u00ean \u0111\u00e2y lu\u00f4n :v</p> <p>L\u1eddi nh\u1eafn nh\u1ee7</p> <p>M\u00ecnh bi\u1ebft l\u00e0 tr\u00ecnh \u0111\u1ed9 c\u1ee7a m\u00ecnh c\u00f2n g\u00e0, n\u00ean nh\u1eefng b\u00e0i post v\u1ec1 c\u00f4ng ngh\u1ec7 c\u1ee7a m\u00ecnh m\u00e0 xu\u1ea5t hi\u1ec7n ki\u1ebfn th\u1ee9c g\u00ec \u0111\u1ea5y sai ho\u1eb7c l\u1ed7i th\u1eddi l\u00e0 chuy\u1ec7n b\u00ecnh th\u01b0\u1eddng. N\u1ebfu m\u00e0 ph\u00e1t hi\u1ec7n ra m\u00ecnh sai ch\u1ed7 n\u00e0o th\u00ec mong b\u1ea1n \u0111\u1ecdc hoan h\u1ec9 nh\u1eafc nh\u1edf m\u00ecnh theo c\u00e1c k\u00eanh contact m\u00ecnh \u0111\u1ec3 g\u00f3c d\u01b0\u1edbi b\u00ean ph\u1ea3i c\u1ee7a page (m\u00ecnh r\u1ea5t mong \u0111i\u1ec1u n\u00e0y x\u1ea3y ra v\u00ec sai \u1edf \u0111\u00e2u th\u00ec b\u1ea3n th\u00e2n n\u00ean bi\u1ebft c\u00e0ng s\u1edbm c\u00e0ng t\u1ed1t \u0111\u1ec3 m\u00e0 s\u1eeda )</p> <p>G\u00f3c xin x\u1ecf</p> <p>Ngo\u00e0i trang blog n\u00e0y th\u00ec m\u00ecnh c\u00f2n m\u1edbi t\u1eadp ch\u01a1i tiktok v\u1edbi m\u1ed9t k\u00eanh v\u1ec1 thu\u1eadt to\u00e1n. Anh em cho xin m\u1ed9t follow \u1ee7ng h\u1ed9 l\u1ea5y tinh th\u1ea7n nh\u00e9 </p> <ul> <li> chikodevn</li> </ul> <p>B\u00e2y gi\u1edd anh em c\u00f3 th\u1ec3 b\u1ea5m v\u00e0o \u0111\u00e2y \u0111\u1ec3 b\u1eaft \u0111\u1ea7u \u0111\u1ecdc c\u00e1c b\u00e0i vi\u1ebft c\u1ee7a m\u00ecnh nh\u00e9. C\u1ea3m \u01a1n anh em nhi\u1ec1u </p>"},{"location":"blog/","title":"Recent Posts","text":""},{"location":"blog/cap-theorem/","title":"The CAP theorem","text":"<p>In this post, we're going to explore what the CAP theorem is and its relevance in today\u2019s distributed systems.</p> <p>To be more intuitive, let's use a real-life example to illustrate the concepts:<sup>1</sup></p> <p>Tiny Bank example</p> <p>Let's say we have a tiny bank with exactly two ATMs connected over a network. The ATMs support three operations: deposit, withdraw, and check balance. There is no central database to keep the account balance, it is stored on both ATMs. No matter what happens, the balance should never go below zero.</p> <p> </p>","tags":["CAP theorem"]},{"location":"blog/cap-theorem/#what-is-the-cap-theorem","title":"What is the CAP theorem?","text":"<p>In database theory, the CAP theorem states that any distributed data store can provide only two of the following three guarantees:</p> <ul> <li> <p>Consistency: Refers to the property of a system where all nodes have a consistent view of the data. It means all clients see the same data at the same time no matter which node they connect to.</p> Tiny Bank example <p>Consistency means that the balance is always the same on both ATMs. When a customer uses an ATM, no matter which one they are using, they always see the same balance.</p> </li> <li> <p>Availability: Refers to the ability of a system to respond to requests from users at all times.</p> Tiny Bank example <p>Availability means that clients can always perform operations at any ATM.</p> </li> <li> <p>Partition tolerance: Refers to the ability of a system to continue operating even if there is a network partition.</p> Tiny Bank example <p>Partition tolerance means that the bank should remain operational even if the two ATMs are unable to communicate with each other.</p> </li> </ul> <p>Note</p> <p>In practice, during a network partition, a system must choose between consistency and availability.</p> <p>If the system prioritizes consistency, it may become unavailable until the partition is resolved.</p> <p>Conversely, if the system prioritizes availability, it may allow updates to the data, potentially resulting in data inconsistencies until the partition is resolved.</p> <p> </p> <p>We will discuss this in more detail in the next section.</p>","tags":["CAP theorem"]},{"location":"blog/cap-theorem/#cap-theorem-nosql-database-types","title":"CAP theorem NoSQL database types","text":"<p>NoSQL databases are ideal for distributed network applications. Unlike their vertically scalable SQL (relational) counterparts, NoSQL databases are horizontally scalable and distributed by design.<sup>2</sup></p> <p>Today, NoSQL databases are classified based on the two CAP characteristics they support:</p>","tags":["CAP theorem"]},{"location":"blog/cap-theorem/#cp-database","title":"CP database","text":"<p>A CP database delivers consistency and partition tolerance at the expense of availability.</p> <p>When a partition occurs between any two nodes, the system has to shut down the non-consistent node (i.e., make it unavailable) until the partition is resolved.</p> Tiny Bank example <p>If there is a network partition and the ATMs are unable to communicate with each other, and the bank prioritizes consistency, the ATMs may refuse to process deposits or withdrawals until the partition is resolved. This ensures that the balance remains consistent, but the system is unavailable to customers.</p> <p> If a network partition occurs, customers cannot make deposits or withdrawals, ensuring the system's consistency. </p>","tags":["CAP theorem"]},{"location":"blog/cap-theorem/#ap-database","title":"AP database","text":"<p>An AP database delivers availability and partition tolerance at the expense of consistency.</p> <p>When a partition occurs, all nodes remain available but those at the wrong end of a partition might return an older version of data than others. (When the partition is resolved, the AP databases typically resync the nodes to repair all inconsistencies in the system.)</p> Tiny Bank example <p>If there is a network partition and the ATMs are unable to communicate with each other, and the bank prioritizes availability, the ATM may allow deposits and withdrawals to occur, but the balance may become inconsistent until the partition is resolved.</p> <p> If a network partition occurs, the balance may become inconsistent when customers make deposits or withdrawals. </p>","tags":["CAP theorem"]},{"location":"blog/cap-theorem/#ca-database","title":"CA database","text":"<p>A CA database delivers consistency and availability across all nodes. It can\u2019t do this if there is a partition between any two nodes in the system. Therefore, it can\u2019t deliver fault tolerance.</p> <p>In practice, this means that for a CA database to function correctly, it must operate in an environment where network partitions are impossible or extremely rare.</p> Tiny Bank example <p>If the system requires both consistency and availability, then network partitions must not occur. If a network partition does happen, the system will become unavailable, users will experience downtime.</p> <p>Note</p> <p>In a distributed system, partitions can\u2019t be avoided. So, while we can discuss a CA distributed database in theory, for all practical purposes, a CA distributed database can\u2019t exist. This doesn\u2019t mean you can\u2019t have a CA database for your distributed application if you need one. Many relational databases, such as PostgreSQL, deliver consistency and availability in a single-node setup or with replication, assuming no network partitions occur.<sup>2</sup></p>","tags":["CAP theorem"]},{"location":"blog/cap-theorem/#summary","title":"Summary","text":"<p>The CAP theorem is a useful tool for understanding the high-level trade-offs to consider during a network partition. While it is a good starting point, it does not provide a complete picture of the trade-offs involved in designing a comprehensive distributed system.</p> <p>Despite numerous advancements in NoSQL databases and other distributed systems, the CAP theorem remains relevant today as a fundamental tradeoff in distributed system designs.<sup>3</sup></p> <p>Note</p> <p>The CAP theorem assumes 100% availability or 100% consistency. In the real world, there are degrees of consistency and availability that distributed system designers must carefully consider.</p> Tiny Bank Example <p>During a network partition, the ATMs could allow only balance inquiries to be processed, while deposits or withdrawals are blocked. This maintains the system's consistency but is not 100% unavailable to customers (customers can still perform balance inquiries).</p> <p> If a network partition occurs, the ATMs could allow only balance inquiries to be processed, while deposits or withdrawals are blocked. </p> <ol> <li> <p>ByteByteGo, CAP Theorem Simplified \u21a9</p> </li> <li> <p>IBM, What is the CAP theorem? \u21a9\u21a9</p> </li> <li> <p>RR, CAP theorem \u2014 Is it still relevant? \u21a9</p> </li> </ol>","tags":["CAP theorem"]},{"location":"blog/hadoop-overview/","title":"Gi\u1edbi thi\u1ec7u v\u1ec1 Apache Hadoop","text":"<p>B\u00e0i vi\u1ebft n\u00e0y gi\u1edbi thi\u1ec7u c\u00e1i nh\u00ecn t\u1ed5ng quan nh\u1ea5t v\u1ec1 Apache Hadoop, bao g\u1ed3m ki\u1ebfn tr\u00fac, c\u00e1ch th\u1ee9c ho\u1ea1t \u0111\u1ed9ng c\u1ee7a n\u00f3.</p> <p>Apache Hadoop l\u00e0 m\u1ed9t framework d\u00f9ng \u0111\u1ec3 l\u01b0u tr\u1eef v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u l\u1edbn. Hadoop s\u1eed d\u1ee5ng m\u1ed9t cluster g\u1ed3m nhi\u1ec1u nodes \u0111\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u song song thay v\u00ec ch\u1ec9 s\u1eed d\u1ee5ng m\u1ed9t m\u00e1y duy nh\u1ea5t, c\u1ea3i thi\u1ec7n t\u1ed1c \u0111\u1ed9 khi x\u1eed l\u00fd d\u1eef li\u1ec7u l\u1edbn.<sup>1</sup></p>","tags":["Hadoop"]},{"location":"blog/hadoop-overview/#tong-quan-ve-hadoop","title":"T\u1ed5ng quan v\u1ec1 Hadoop","text":"<p>Hadoop s\u1eed d\u1ee5ng h\u00e0ng tr\u0103m th\u1eadm ch\u00ed h\u00e0ng ng\u00e0n servers l\u00e0m vi\u1ec7c c\u00f9ng nhau \u0111\u1ec3 l\u01b0u tr\u1eef v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u l\u1edbn.</p> <p>Hadoop bao g\u1ed3m 4 modules ch\u00ednh<sup>1</sup>:</p> <ul> <li>Hadoop Distributed File System (HDFS) l\u00e0 m\u1ed9t distributed file system, d\u00f9ng \u0111\u1ec3 l\u01b0u tr\u1eef data.</li> <li>Yet Another Resource Negotiator (YARN) l\u00e0 module \u0111\u1ec3 qu\u1ea3n l\u00fd v\u00e0 monitor c\u00e1c nodes. N\u00f3 c\u00f3 t\u00e1c d\u1ee5ng schedules c\u00e1c jobs v\u00e0 tasks, \u0111\u00f3ng vai tr\u00f2 nh\u01b0 m\u1ed9t resource manager.</li> <li>MapReduce l\u00e0 module th\u1ef1c thi\u1ec7n x\u1eed l\u00fd data. </li> <li>Hadoop Common cung c\u1ea5p c\u00e1c th\u01b0 vi\u1ec7n java \u0111\u1ec3 c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng \u1edf c\u00e1c module kh\u00e1c.</li> </ul> <p>M\u1ed9t Hadoop cluster bao g\u1ed3m m\u1ed9t ho\u1eb7c nhi\u1ec1u master nodes v\u00e0 nhi\u1ec1u slave nodes, c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c scale out b\u1eb1ng c\u00e1ch th\u00eam nodes v\u00e0o cluster.</p> <p>C\u00e1c ph\u1ea7n ti\u1ebfp theo c\u1ee7a b\u00e0i vi\u1ebft s\u1ebd n\u00f3i k\u1ef9 h\u01a1n v\u1ec1 ki\u1ebfn tr\u00fac v\u00e0 c\u00e1ch ho\u1ea1t \u0111\u1ed9ng c\u1ee7a Hadoop.</p>","tags":["Hadoop"]},{"location":"blog/hadoop-overview/#kien-truc-hadoop","title":"Ki\u1ebfn tr\u00fac Hadoop","text":"<p>Ki\u1ebfn tr\u00fac c\u1ee7a Hadoop c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c chia th\u00e0nh 4 layers:</p> T\u1ed5ng quan v\u1ec1 ki\u1ebfn tr\u00fac c\u1ee7a Hadoop","tags":["Hadoop"]},{"location":"blog/hadoop-overview/#distributed-storage-layer","title":"Distributed Storage Layer","text":"<p>Th\u1eb1ng n\u00e0y ch\u00ednh l\u00e0 th\u1eb1ng HDFS, bao g\u1ed3m m\u1ed9t ho\u1eb7c nhi\u1ec1u master nodes (hay c\u00f2n g\u1ecdi l\u00e0 NameNode) v\u00e0 nhi\u1ec1u slave nodes (hay c\u00f2n g\u1ecdi l\u00e0 DataNode). M\u1ed7i node c\u00f3 b\u1ed9 nh\u1edb c\u1ee7a ri\u00eang n\u00f3. Data \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o s\u1ebd \u0111\u01b0\u1ee3c chia th\u00e0nh c\u00e1c data blocks sau \u0111\u00f3 \u0111\u01b0\u1ee3c l\u01b0u \u1edf HDFS distributed storage layer. Ngo\u00e0i ra, HDFS l\u01b0u th\u00eam 3 b\u1ea3n copies c\u1ee7a data \u1edf tr\u00ean kh\u1eafp cluster. NameNode s\u1ebd l\u01b0u th\u00f4ng tin v\u1ec1 c\u00e1c data block c\u1ee5 th\u1ec3 v\u00e0 c\u00e1c replicas c\u1ee7a n\u00f3 \u0111\u01b0\u1ee3c l\u01b0u \u1edf \u0111\u00e2u trong cluster.</p>","tags":["Hadoop"]},{"location":"blog/hadoop-overview/#cluster-resource-management","title":"Cluster Resource Management","text":"<p>Th\u1eb1ng n\u00e0y ch\u00ednh l\u00e0 th\u1eb1ng YARN. N\u00f3 s\u1ebd ch\u1ec9 \u0111\u1ecbnh resource cho c\u00e1c frameworks kh\u00e1c \u0111\u01b0\u1ee3c vi\u1ebft cho Hadoop. M\u1ed9t s\u1ed1 framework nh\u01b0 Apache Pig, Hive, Giraph, Zookeeper. N\u00f3 c\u0169ng ch\u1ec9 \u0111\u1ecbnh resource cho ch\u00ednh th\u1eb1ng MapReduce lu\u00f4n.</p>","tags":["Hadoop"]},{"location":"blog/hadoop-overview/#processing-framework-layer","title":"Processing Framework Layer","text":"<p>Layer n\u00e0y bao g\u1ed3m c\u00e1c frameworks th\u1ef1c hi\u1ec7n c\u00e1c b\u01b0\u1edbc x\u1eed l\u00fd data. C\u00e1c framework nh\u01b0 Spark, Storm hay Tez b\u00e2y gi\u1edd c\u00f3 th\u1ec3 x\u1eed l\u00fd real-time, t\u0103ng hi\u1ec7u qu\u1ea3 cho h\u1ec7 th\u1ed1ng.</p>","tags":["Hadoop"]},{"location":"blog/hadoop-overview/#application-programming-interface","title":"Application Programming Interface","text":"<p>Nh\u01b0 t\u00ean g\u1ecdi c\u1ee7a n\u00f3, layer n\u00e0y g\u1ed3m c\u00e1c API \u0111\u1ec3 c\u00e1c l\u1eadp tr\u00ecnh vi\u00ean s\u1eed d\u1ee5ng.</p> <p>Chi ti\u1ebft v\u1ec1 c\u00e1ch ho\u1ea1t \u0111\u1ed9ng c\u1ee7a t\u1eebng th\u00e0nh ph\u1ea7n s\u1ebd \u0111\u01b0\u1ee3c n\u00f3i k\u1ef9 h\u01a1n \u1edf m\u1ed9t b\u00e0i vi\u1ebft kh\u00e1c.</p> <ol> <li> <p>AWS, What is Hadoop \u21a9\u21a9</p> </li> </ol>","tags":["Hadoop"]},{"location":"blog/load-balancing-algorithms/","title":"Load Balancing Algorithms","text":"<p>In this post, we'll provide a comprehensive overview of load balancing algorithms, discussing how they work and their pros and cons.</p> <p>A load balancer is a device that distributes network traffic across multiple servers. A load balancing algorithm is the logic a load balancer uses to distribute network traffic among these servers.<sup>1</sup></p> <p>There are two main categories of algorithms: static and dynamic. Let's explore each category and dive deeper into the major specific algorithms.<sup>2</sup></p>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#static-algorithms","title":"Static Algorithms","text":"<p>Static load balancing algorithms distribute requests to servers without taking into account the servers' real-time conditions and performance metrics.</p>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#round-robin","title":"Round Robin","text":"<p>Description</p> <p>Distributes requests evenly among servers in sequence.</p> <p>Pros</p> <ul> <li>Easy to implement and understand.</li> </ul> <p>Cons</p> <ul> <li>Can potentially overload servers if they are not properly monitored.</li> </ul>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#sticky-round-robin","title":"Sticky Round Robin","text":"<p>Description</p> <p>An extension of round robin that tries to route subsequent requests from the same user to the same server.</p> <p>Pros</p> <ul> <li>Improves performance by keeping related data on the same server.</li> </ul> <p>Cons</p> <ul> <li>Uneven loads can easily occur since newly arriving users are assigned randomly.</li> </ul>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#weighted-round-robin","title":"Weighted Round Robin","text":"<p>Description</p> <p>Allows admins to assign different weights or priorities to different servers. Servers with higher weights receive higher number of requests.</p> <p>Pros</p> <ul> <li>Accounts for heterogeneous server capabilities.</li> </ul> <p>Cons</p> <ul> <li>Weights must be manually configured, which is less adaptive to real-time changes.</li> </ul>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#hash-based-algorithms","title":"Hash-Based Algorithms","text":"<p>Description</p> <p>Uses a hash function to map incoming requests to the backend servers. The hash function often uses the client's IP address or the requested URL as input for determining where to route each request.</p> <p>Pros</p> <ul> <li>Can evenly distribute requests if the function is chosen wisely.</li> </ul> <p>Cons</p> <ul> <li>Selecting an optimal hash function can be challenging.</li> </ul>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#dynamic-algorithms","title":"Dynamic Algorithms","text":"<p>Dynamic load balancing algorithms adapt in real-time by taking active performance metrics and server conditions into account when distributing requests.</p>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#least-connections","title":"Least Connections","text":"<p>Description</p> <p>Sends each new request to the server currently with the least number of active connections or open requests.</p> <p>Pros</p> <ul> <li>New requests are adaptively routed to where there is the most remaining capacity.</li> </ul> <p>Cons</p> <ul> <li>Requires actively tracking the number of ongoing connections on each backend server.</li> <li>Load can unintentionally concentrate on certain servers if connections pile up unevenly.</li> </ul>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#least-response-time","title":"Least Response Time","text":"<p>Description</p> <p>Sends incoming requests to the server with the lowest current latency or fastest response time. Latency for each server is continuously measured and factored in.</p> <p>Pros</p> <ul> <li>Highly adaptive and reactive.</li> </ul> <p>Cons</p> <ul> <li>Requires constant monitoring, which incurs significant overhead and introduces complexity.</li> <li>Does not consider how many existing requests each server already has.</li> </ul>","tags":["Load Balancing"]},{"location":"blog/load-balancing-algorithms/#summary","title":"Summary","text":"<p>There are clear tradeoffs between simple static algorithms and more adaptive dynamic ones. Static algorithms like round robin work well for stateless applications. Dynamic algorithms help optimize response times and availability for large, complex applications.<sup>2</sup></p> <ol> <li> <p>Cloudflare, Types of load balancing algorithms \u21a9</p> </li> <li> <p>ByteByteGo, Top 6 Load Balancing Algorithms Every Developer Should Know \u21a9\u21a9</p> </li> </ol>","tags":["Load Balancing"]},{"location":"course/","title":"Courses","text":""},{"location":"course/ml-algorithms/","title":"Machine Learning Algorithms","text":"<ul> <li>Decision Tree</li> </ul>"},{"location":"course/ml-algorithms/decision-tree/","title":"Decision Tree","text":"<p>B\u00e0i n\u00e0y s\u1ebd n\u00f3i v\u1ec1 thu\u1eadt to\u00e1n Decision Tree v\u00e0 th\u1ef1c h\u00e0nh n\u00f3 tr\u00ean Python.</p> <p>\u0110\u1ecdc b\u00e0i g\u1ed1c \u1edf b\u00e0i vi\u1ebft tr\u00ean forum Machine Learning c\u01a1 b\u1ea3n.</p>"},{"location":"course/ml-algorithms/decision-tree/#bai-toan","title":"B\u00e0i to\u00e1n","text":"<p>T\u1eeb m\u1ed9t b\u1ea3ng d\u1eef li\u1ec7u, ta s\u1ebd x\u00e2y d\u1ef1ng m\u1ed9t Decision Tree \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n nh\u00e3n c\u1ee7a m\u1ed9t \u0111i\u1ec3m d\u1eef li\u1ec7u m\u1edbi khi bi\u1ebft c\u00e1c thu\u1ed9c t\u00ednh c\u1ee7a \u0111i\u1ec3m d\u1eef li\u1ec7u \u0111\u00f3.</p> Th\u1ef1c h\u00e0nh <p>Cho m\u1ed9t b\u1ea3ng d\u1eef li\u1ec7u ghi ch\u00e9p l\u1ea1i d\u1eef li\u1ec7u th\u1eddi ti\u1ebft c\u1ee7a 14 ng\u00e0y, c\u00f9ng v\u1edbi \u0111\u00f3 l\u00e0 vi\u1ec7c m\u1ed9t \u0111\u1ed9i b\u00f3ng c\u00f3 quy\u1ebft \u0111\u1ecbnh \u0111i ch\u01a1i b\u00f3ng hay kh\u00f4ng:</p> B\u1ea3ng d\u1eef li\u1ec7u <code>weather.csv</code> id outlook temperature humidity wind play 1 sunny hot high weak no 2 sunny hot high strong no 3 overcast hot high weak yes 4 rainy mild high weak yes 5 rainy cool normal weak yes 6 rainy cool normal strong no 7 overcast cool normal strong yes 8 sunny mild high weak no 9 sunny cool normal weak yes 10 rainy mild normal weak yes 11 sunny mild normal strong yes 12 overcast mild high strong yes 13 overcast hot normal weak yes 14 rainy mild high strong no <p>T\u1eeb b\u1ea3ng d\u1eef li\u1ec7u n\u00e0y, ta s\u1ebd x\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh Decision Tree \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n \u0111\u1ed9i b\u00f3ng n\u00e0y c\u00f3 \u0111i ch\u01a1i b\u00f3ng hay kh\u00f4ng n\u1ebfu bi\u1ebft d\u1eef li\u1ec7u th\u1eddi ti\u1ebft c\u1ee7a ng\u00e0y h\u00f4m \u0111\u00f3. N\u00f3i c\u00e1ch kh\u00e1c, ta c\u1ea7n d\u1ef1 \u0111o\u00e1n gi\u00e1 tr\u1ecb c\u1ee7a play khi bi\u1ebft gi\u00e1 tr\u1ecb c\u1ee7a 4 thu\u1ed9c t\u00ednh l\u00e0 outlook, temperature, humidity, wind.</p>"},{"location":"course/ml-algorithms/decision-tree/#xay-dung-decision-tree","title":"X\u00e2y d\u1ef1ng Decision Tree","text":"<p>\u0110\u1ea7u ti\u00ean, v\u1edbi b\u1ea3ng d\u1eef li\u1ec7u ban \u0111\u1ea7u, ta c\u1ea7n t\u00ecm m\u1ed9t thu\u1ed9c t\u00ednh \u0111\u1ec3 chia d\u1eef li\u1ec7u th\u00e0nh c\u00e1c nh\u00f3m kh\u00e1c nhau, m\u1ed7i nh\u00f3m t\u01b0\u01a1ng \u1ee9ng v\u1edbi m\u1ed9t gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh \u0111\u00f3.</p> Th\u1ef1c h\u00e0nh <p>\u0110\u1ed1i v\u1edbi b\u00e0i to\u00e1n c\u1ee5 th\u1ec3 n\u00f3i tr\u00ean, ta c\u1ea7n t\u00ecm m\u1ed9t trong s\u1ed1 4 thu\u1ed9c t\u00ednh l\u00e0 outlook, temperature, humidity, wind \u0111\u1ec3 chia d\u1eef li\u1ec7u th\u00e0nh c\u00e1c nh\u00f3m kh\u00e1c nhau, m\u1ed7i nh\u00f3m t\u01b0\u01a1ng \u1ee9ng v\u1edbi m\u1ed9t gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh \u0111\u00f3.</p> <p>V\u1eady trong c\u00e1c thu\u1ed9c t\u00ednh c\u1ee7a m\u1ed9t b\u1ea3ng d\u1eef li\u1ec7u, l\u00e0m sao \u0111\u1ec3 bi\u1ebft ta n\u00ean chia theo thu\u1ed9c t\u00ednh n\u00e0o?</p> <p>C\u00e2u tr\u1ea3 l\u1eddi l\u00e0 ta s\u1ebd \u0111i t\u00ednh information gain c\u1ee7a t\u1eebng thu\u1ed9c t\u00ednh, sau \u0111\u00f3 ch\u1ecdn ra thu\u1ed9c t\u00ednh c\u00f3 information gain l\u1edbn nh\u1ea5t r\u1ed3i chia theo thu\u1ed9c t\u00ednh \u0111\u00f3.</p> <p>C\u00f4ng th\u1ee9c t\u00ednh information gain c\u1ee7a m\u1ed9t thu\u1ed9c t\u00ednh \\(x\\) trong b\u1ea3ng d\u1eef li\u1ec7u \\(\\mathcal{S}\\) nh\u01b0 sau:</p> \\[ \\tag{1} G(x, \\mathcal{S}) = H(\\mathcal{S}) - H(x, \\mathcal{S}) \\] <p>trong \u0111\u00f3:</p> <ul> <li>\\(H(\\mathcal{S})\\) l\u00e0 entropy c\u1ee7a b\u1ea3ng d\u1eef li\u1ec7u \\(\\mathcal{S}\\)</li> <li>\\(H(x, \\mathcal{S})\\) l\u00e0 weighted entropy c\u1ee7a b\u1ea3ng d\u1eef li\u1ec7u \\(\\mathcal{S}\\) sau khi \u0111\u01b0\u1ee3c chia theo thu\u1ed9c t\u00ednh \\(x\\)</li> </ul> <p>V\u1eady th\u00ec 2 c\u00e1i n\u00e0y \u0111\u01b0\u1ee3c t\u00ednh nh\u01b0 th\u1ebf n\u00e0o?</p>"},{"location":"course/ml-algorithms/decision-tree/#entropy","title":"Entropy","text":"<p>Cho b\u1ea3ng d\u1eef li\u1ec7u \\(\\mathcal{S}\\), entropy c\u1ee7a n\u00f3 \u0111\u01b0\u1ee3c t\u00ednh nh\u01b0 sau:</p> \\[ \\tag{2} H(\\mathcal{S}) = - \\sum_{c \\in \\mathbf{C}} \\dfrac{N_c}{N} \\log_2 \\left( \\dfrac{N_c}{N} \\right) \\] <p>trong \u0111\u00f3:</p> <ul> <li>\\(\\mathbf{C}\\) l\u00e0 t\u1eadp c\u00e1c class c\u1ee7a b\u00e0i to\u00e1n</li> <li>\\(N_c\\) l\u00e0 s\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u thu\u1ed9c class \\(c\\) trong b\u1ea3ng d\u1eef li\u1ec7u \\(\\mathcal{S}\\)</li> <li>\\(N\\) l\u00e0 t\u1ed5ng s\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u trong b\u1ea3ng d\u1eef li\u1ec7u \\(\\mathcal{S}\\)</li> </ul> <p>L\u01b0u \u00fd</p> <ul> <li>Ph\u00e9p \\(\\log\\) l\u00e0 l\u1ea5y theo c\u01a1 s\u1ed1 \\(2\\).</li> <li>Quy \u01b0\u1edbc: \\(0 \\log 0 = 0\\)</li> </ul> Th\u1ef1c h\u00e0nh <p>V\u1edbi b\u1ea3ng d\u1eef li\u1ec7u \u0111\u00e3 cho, ta c\u00f3:</p> <ul> <li>\\(\\mathbf{C} = \\left\\{ \\texttt{yes}, \\texttt{no} \\right\\}\\)</li> <li>\\(N = 14\\)</li> <li>\\(N_{\\texttt{yes}} = 9\\); \\(N_{\\texttt{no}} = 5\\)</li> </ul> <p>Do \u0111\u00f3, ta t\u00ednh \u0111\u01b0\u1ee3c \\(H(\\mathcal{S})\\):</p> \\[ H(\\mathcal{S}) = - \\frac{5}{14}\\log\\left(\\frac{5}{14}\\right) - \\frac{9}{14}\\log\\left(\\frac{9}{14}\\right) \\approx 0.9403 \\]"},{"location":"course/ml-algorithms/decision-tree/#weighted-entropy","title":"Weighted Entropy","text":"<p>Cho b\u1ea3ng d\u1eef li\u1ec7u \\(\\mathcal{S}\\) v\u00e0 m\u1ed9t thu\u1ed9c t\u00ednh \\(x\\), weighted entropy c\u1ee7a \\(\\mathcal{S}\\) khi chia theo thu\u1ed9c t\u00ednh \\(x\\) nh\u01b0 sau:</p> \\[ \\tag{3} H(x, \\mathcal{S}) = \\sum_{k \\in K} \\frac{m_k}{N} H(\\mathcal{S}_k) \\] <p>trong \u0111\u00f3:</p> <ul> <li>\\(K\\) l\u00e0 t\u1eadp c\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh \\(x\\)</li> <li>\\(N\\) l\u00e0 t\u1ed5ng s\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u trong b\u1ea3ng d\u1eef li\u1ec7u \\(\\mathcal{S}\\)</li> <li>\\(\\mathcal{S}_k\\) l\u00e0 b\u1ea3ng d\u1eef li\u1ec7u m\u1edbi thu \u0111\u01b0\u1ee3c khi ch\u1ec9 l\u1ea5y c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u c\u00f3 \\(x = k\\)</li> <li>\\(m_k\\) l\u00e0 t\u1ed5ng s\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u c\u00f3 \\(x = k\\). N\u00f3i c\u00e1ch kh\u00e1c, \\(m_k\\) l\u00e0 t\u1ed5ng s\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u c\u1ee7a \\(\\mathcal{S}_k\\)</li> </ul> Th\u1ef1c h\u00e0nh <p>V\u1edbi b\u1ea3ng d\u1eef li\u1ec7u ban \u0111\u1ea7u, x\u00e9t thu\u1ed9c t\u00ednh outlook, c\u00f3 3 gi\u00e1 tr\u1ecb l\u00e0 sunny, overcast, rainy. T\u01b0\u01a1ng \u1ee9ng ta c\u00f3 3 b\u1ea3ng d\u1eef li\u1ec7u m\u1edbi l\u00e0 \\(\\mathcal{S}_s\\), \\(\\mathcal{S}_o\\), \\(\\mathcal{S}_r\\):</p> \\(\\mathcal{S}_s\\)\\(\\mathcal{S}_o\\)\\(\\mathcal{S}_r\\) id outlook temperature humidity wind play 1 sunny hot high weak no 2 sunny hot high strong no 8 sunny mild high weak no 9 sunny cool normal weak yes 11 sunny mild normal strong yes id outlook temperature humidity wind play 3 overcast hot high weak yes 7 overcast cool normal strong yes 12 overcast mild high strong yes 13 overcast hot normal weak yes id outlook temperature humidity wind play 4 rainy mild high weak yes 5 rainy cool normal weak yes 6 rainy cool normal strong no 10 rainy mild normal weak yes 14 rainy mild high strong no <p>S\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u c\u1ee7a 3 b\u1ea3ng d\u1eef li\u1ec7u n\u00e0y l\u1ea7n l\u01b0\u1ee3t l\u00e0: \\(m_s = 5\\), \\(m_o = 4\\), \\(m_r = 5\\).</p> <p>Ta t\u00ednh \u0111\u01b0\u1ee3c entropy c\u1ee7a 3 b\u1ea3ng d\u1eef li\u1ec7u n\u00e0y nh\u01b0 sau:</p> \\[ \\begin{eqnarray}     H(\\mathcal{S}_s) &amp;=&amp;-\\frac{3}{5}\\log\\left(\\frac{3}{5}\\right) - \\frac{2}{5}\\log\\left(\\frac{2}{5}\\right) \\approx 0.9710 \\\\     H(\\mathcal{S}_o) &amp;=&amp;-\\frac{0}{4}\\log\\left(\\frac{0}{4}\\right) - \\frac{4}{4}\\log\\left(\\frac{4}{4}\\right) = 0\\\\     H(\\mathcal{S}_r) &amp;=&amp; -\\frac{2}{5}\\log\\left(\\frac{2}{5}\\right) - \\frac{3}{5}\\log\\left(\\frac{3}{5}\\right) \\approx 0.9710 \\end{eqnarray} \\] <p>T\u1eeb \u0111\u00f3 ta t\u00ednh \u0111\u01b0\u1ee3c weighted entropy c\u1ee7a \\(\\mathcal{S}\\) khi chia theo thu\u1ed9c t\u00ednh outlook nh\u01b0 sau:</p> \\[     H({outlook}, \\mathcal{S}) = \\frac{5}{14}H(\\mathcal{S}_s) + \\frac{4}{14}H(\\mathcal{S}_o) + \\frac{5}{14}H(\\mathcal{S}_r) \\approx 0.6935 \\] <p>T\u01b0\u01a1ng t\u1ef1, ta c\u00f3 th\u1ec3 t\u00ednh \u0111\u01b0\u1ee3c weighted entropy cho c\u00e1c thu\u1ed9c t\u00ednh c\u00f2n l\u1ea1i:</p> \\[ H({temperature, \\mathcal{S}}) \\approx 0.9111, \\quad H(humidity, \\mathcal{S}) \\approx 0.7885, \\quad H(wind, \\mathcal{S}) \\approx 0.8922 \\]"},{"location":"course/ml-algorithms/decision-tree/#information-gain","title":"Information Gain","text":"<p>Sau khi t\u00ednh \u0111\u01b0\u1ee3c entropy c\u1ee7a b\u1ea3ng d\u1eef li\u1ec7u v\u00e0 weighted entropy c\u1ee7a t\u1eebng thu\u1ed9c t\u00ednh, ta t\u00ednh information gain c\u1ee7a t\u1eebng thu\u1ed9c t\u00ednh theo c\u00f4ng th\u1ee9c \\((1)\\). Ch\u1ecdn ra thu\u1ed9c t\u00ednh c\u00f3 information gain l\u1edbn nh\u1ea5t v\u00e0 chia b\u1ea3ng d\u1eef li\u1ec7u theo thu\u1ed9c t\u00ednh \u0111\u00f3 \u0111\u1ec3 \u0111\u01b0\u1ee3c c\u00e1c b\u1ea3ng d\u1eef li\u1ec7u m\u1edbi nh\u1ecf h\u01a1n.</p> <p>D\u1ec5 th\u1ea5y thu\u1ed9c t\u00ednh c\u00f3 information gain l\u1edbn nh\u1ea5t th\u00ec ch\u00ednh l\u00e0 thu\u1ed9c t\u00ednh c\u00f3 weighted entropy nh\u1ecf nh\u1ea5t.</p> <p>\u1ede \u0111\u00e2y b\u1ea3ng d\u1eef li\u1ec7u ban \u0111\u1ea7u t\u01b0\u01a1ng \u1ee9ng v\u1edbi m\u1ed9t node c\u1ee7a Decision Tree, c\u00e1c b\u1ea3ng d\u1eef li\u1ec7u con t\u01b0\u01a1ng \u1ee9ng v\u1edbi c\u00e1c node con c\u1ee7a node \u0111\u00f3.</p> <p>V\u1edbi m\u1ed7i b\u1ea3ng d\u1eef li\u1ec7u t\u01b0\u01a1ng \u1ee9ng v\u1edbi c\u00e1c node con, ta ti\u1ebfp t\u1ee5c khi n\u00f3 th\u00e0nh c\u00e1c d\u1eef li\u1ec7u nh\u1ecf h\u01a1n cho \u0111\u1ebfn khi b\u1ea3ng d\u1eef li\u1ec7u thu \u0111\u01b0\u1ee3c ch\u1ec9 c\u00f3 \u0111\u00fang 1 class (node l\u00e1), hay b\u1ea3ng d\u1eef li\u1ec7u thu \u0111\u01b0\u1ee3c c\u00f3 entropy b\u1eb1ng \\(0\\)</p> Th\u1ef1c h\u00e0nh <p>V\u1edbi b\u1ea3ng d\u1eef li\u1ec7u ban \u0111\u1ea7u, ta th\u1ea5y outlook l\u00e0 thu\u1ed9c t\u00ednh c\u00f3 weighted entropy nh\u1ecf nh\u1ea5t, nh\u01b0 v\u1eady ta s\u1ebd ch\u1ecdn thu\u1ed9c t\u00ednh n\u00e0y \u0111\u1ea7u ti\u00ean cho Decision Tree.</p> <p>B\u1eaft \u0111\u1ea7u v\u1edbi b\u1ea3ng d\u1eef li\u1ec7u ban \u0111\u1ea7u t\u01b0\u01a1ng \u1ee9ng v\u1edbi root node, ta \u0111\u01b0\u1ee3c 3 b\u1ea3ng d\u1eef li\u1ec7u con t\u01b0\u01a1ng \u1ee9ng v\u1edbi 3 node con.</p> <p>Ti\u1ebfp t\u1ee5c th\u1ef1c hi\u1ec7n c\u00e1c b\u01b0\u1edbc tr\u00ean \u1edf c\u00e1c b\u1ea3ng d\u1eef li\u1ec7u con cho \u0111\u1ebfn khi thu \u0111\u01b0\u1ee3c b\u1ea3ng d\u1eef li\u1ec7u c\u00f3 entropy b\u1eb1ng 0, ta \u0111\u01b0\u1ee3c h\u00ecnh d\u1ea1ng c\u1ee7a Decision Tree cho b\u00e0i to\u00e1n n\u00e0y nh\u01b0 sau<sup>1</sup>:</p> <p> </p>"},{"location":"course/ml-algorithms/decision-tree/#ieu-kien-dung","title":"\u0110i\u1ec1u ki\u1ec7n d\u1eebng","text":"<p>Trong thu\u1eadt to\u00e1n tr\u00ean, ta li\u00ean t\u1ee5c chia c\u00e1c node cho \u0111\u1ebfn khi \u0111\u01b0\u1ee3c node l\u00e1. K\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng s\u1ebd thu \u0111\u01b0\u1ee3c m\u1ed9t tree m\u00e0 m\u1ecdi \u0111i\u1ec3m trong t\u1eadp train \u0111\u1ec1u \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n \u0111\u00fang. L\u00fac n\u00e0y tree s\u1ebd r\u1ea5t ph\u1ee9c t\u1ea1p, nhi\u1ec1u node l\u00e1 ch\u1ec9 c\u00f3 m\u1ed9t v\u00e0i \u0111i\u1ec3m d\u1eef li\u1ec7u. Nh\u01b0 v\u1eady, t\u00ecnh tr\u1ea1ng overfitting r\u1ea5t d\u1ec5 x\u1ea3y ra.</p> <p>\u0110\u1ec3 tr\u00e1nh t\u00ecnh tr\u1ea1ng tr\u00ean, c\u00f3 m\u1ed9t s\u1ed1 ph\u01b0\u01a1ng ph\u00e1p \u0111\u1ec3 ki\u1ec3m tra \u0111i\u1ec1u ki\u1ec7n d\u1eebng. N\u1ebfu m\u1ed9t trong s\u1ed1 c\u00e1c \u0111i\u1ec1u ki\u1ec7n n\u00e0y x\u1ea3y ra, ta s\u1ebd kh\u00f4ng ti\u1ebfp t\u1ee5c ph\u00e2n chia node \u0111\u00f3 v\u00e0 coi n\u00f3 l\u00e0 m\u1ed9t node l\u00e1:</p> <ul> <li>N\u1ebfu node \u0111\u00f3 c\u00f3 entropy b\u1eb1ng 0, t\u1ee9c m\u1ecdi \u0111i\u1ec3m trong node \u0111\u1ec1u thu\u1ed9c m\u1ed9t class</li> <li>N\u1ebfu node \u0111\u00f3 c\u00f3 s\u1ed1 ph\u1ea7n t\u1eed nh\u1ecf h\u01a1n m\u1ed9t ng\u01b0\u1ee1ng n\u00e0o \u0111\u00f3</li> <li>N\u1ebfu kho\u1ea3ng c\u00e1ch t\u1eeb node \u0111\u00f3 \u0111\u1ebfn root node \u0111\u1ea1t t\u1edbi m\u1ed9t gi\u00e1 tr\u1ecb n\u00e0o \u0111\u00f3</li> <li>N\u1ebfu t\u1ed5ng s\u1ed1 node l\u00e1 v\u01b0\u1ee3t qua m\u1ed9t ng\u01b0\u1ee1ng n\u00e0o \u0111\u00f3</li> <li>N\u1ebfu vi\u1ec7c ph\u00e2n chia node \u0111\u00f3 kh\u00f4ng l\u00e0m gi\u1ea3m entropy qu\u00e1 nhi\u1ec1u (information gain nh\u1ecf h\u01a1n m\u1ed9t ng\u01b0\u1ee1ng n\u00e0o \u0111\u00f3)</li> </ul> <p>Khi s\u1eed d\u1ee5ng c\u00e1c ph\u01b0\u01a1ng ph\u00e1p tr\u00ean, ta ch\u1ea5p nh\u1eadn vi\u1ec7c c\u00f3 m\u1ed9t s\u1ed1 \u0111i\u1ec3m trong t\u1eadp train b\u1ecb ph\u00e2n l\u1edbp sai \u0111\u1ec3 tr\u00e1nh overfitting.</p> <p>Ngo\u00e0i c\u00e1c ph\u01b0\u01a1ng ph\u00e1p tr\u00ean, m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p ph\u1ed5 bi\u1ebfn kh\u00e1c \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 tr\u00e1nh overfitting l\u00e0 pruning.</p>"},{"location":"course/ml-algorithms/decision-tree/#pruning","title":"Pruning","text":"<p>Note</p> <p>Ph\u1ea7n n\u00e0y ch\u1ec9 tr\u00ecnh b\u00e0y \u00fd t\u01b0\u1edfng c\u1ee7a ph\u01b0\u01a1ng ph\u00e1p pruning ch\u1ee9 kh\u00f4ng \u0111i v\u00e0o chi ti\u1ebft.</p> <p>\u00dd t\u01b0\u1edfng chung c\u1ee7a ph\u01b0\u01a1ng ph\u00e1p pruning l\u00e0 \u0111\u1ea7u ti\u00ean, x\u00e2y d\u1ef1ng m\u1ed9t decision tree trong \u0111\u00f3 m\u1ecdi \u0111i\u1ec3m trong t\u1eadp train \u0111\u1ec1u \u0111\u01b0\u1ee3c ph\u00e2n l\u1edbp \u0111\u00fang. Sau \u0111\u00f3, c\u00e1c node l\u00e1 c\u00f3 chung m\u1ed9t node cha s\u1ebd \u0111\u01b0\u1ee3c c\u1eaft t\u1ec9a v\u00e0 node cha \u0111\u00f3 s\u1ebd tr\u1edf th\u00e0nh m\u1ed9t node l\u00e1.</p> <p>M\u1ed9t s\u1ed1 c\u00e1ch \u0111\u1ec3 pruning nh\u01b0 sau:</p> <ol> <li> <p>Tr\u01b0\u1edbc ti\u00ean dataset \u0111\u01b0\u1ee3c chia th\u00e0nh m\u1ed9t t\u1eadp train v\u00e0 t\u1eadp validation. Decision tree \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng tr\u00ean t\u1eadp train cho t\u1edbi khi m\u1ecdi \u0111i\u1ec3m trong t\u1eadp train \u0111\u1ec1u \u0111\u01b0\u1ee3c ph\u00e2n l\u1edbp \u0111\u00fang. Sau \u0111\u00f3 \u0111i ng\u01b0\u1ee3c t\u1eeb c\u00e1c node l\u00e1, c\u1eaft t\u1ec9a c\u00e1c node anh em c\u1ee7a n\u00f3 (c\u00e1c node c\u00f3 c\u00f9ng node cha) n\u1ebfu nh\u01b0 \u0111\u1ed9 ch\u00ednh x\u00e1c tr\u00ean t\u1eadp validation \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n. Khi n\u00e0o \u0111\u1ed9 ch\u00ednh x\u00e1c tr\u00ean t\u1eadp validation kh\u00f4ng c\u1ea3i thi\u1ec7n \u0111\u01b0\u1ee3c n\u1eefa th\u00ec d\u1eebng l\u1ea1i.</p> </li> <li> <p>S\u1eed d\u1ee5ng to\u00e0n b\u1ed9 t\u1eadp train \u0111\u1ec3 x\u00e2y d\u1ef1ng decision tree. Gi\u1ea3 s\u1eed decision tree cu\u1ed1i c\u00f9ng c\u00f3 \\(K\\) node l\u00e1, t\u1eadp h\u1ee3p c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u \u1edf m\u1ed7i node l\u00e1 l\u1ea7n l\u01b0\u1ee3t l\u00e0 \\(\\mathcal{S}_1, \\dots, \\mathcal{S}_K\\), ta \u0111\u1ecbnh ngh\u0129a h\u00e0m loss sau:    $$     \\tag{4}     \\mathcal{L} = \\sum_{k = 1}^K \\frac{|\\mathcal{S}_k|}{|\\mathcal{S}|} H(\\mathcal{S}_k) + \\lambda K, \\quad \\lambda \\in \\mathbb{R}^+     $$ \u0110\u00e2y ch\u00ednh l\u00e0 k\u1ef9 thu\u1eadt regularization. Gi\u00e1 tr\u1ecb c\u1ee7a h\u00e0m s\u1ed1 n\u00e0y nh\u1ecf n\u1ebfu c\u1ea3 data loss (s\u1ed1 h\u1ea1ng th\u1ee9 nh\u1ea5t) nh\u1ecf (entropy t\u1ea1i m\u1ed7i node l\u00e0 th\u1ea5p) v\u00e0 regularization (s\u1ed1 h\u1ea1ng th\u1ee9 hai) c\u0169ng nh\u1ecf (s\u1ed1 node l\u00e1 \u00edt). Tr\u01b0\u1edbc h\u1ebft, x\u00e2y d\u1ef1ng m\u1ed9t decision tree m\u00e0 m\u1ecdi \u0111i\u1ec3m trong t\u1eadp train \u0111\u1ec1u \u0111\u01b0\u1ee3c ph\u00e2n lo\u1ea1i \u0111\u00fang (to\u00e0n b\u1ed9 c\u00e1c entopy c\u1ee7a c\u00e1c node b\u1eb1ng 0). L\u00fac n\u00e0y data loss b\u1eb1ng \\(0\\) nh\u01b0ng regularization c\u00f3 th\u1ec3 l\u1edbn, khi\u1ebfn cho \\(\\mathcal{L}\\) l\u1edbn. Sau \u0111\u00f3, ta t\u1ec9a d\u1ea7n c\u00e1c node l\u00e1 sao cho \\(\\mathcal{L}\\) gi\u1ea3m. Vi\u1ec7c c\u1eaft t\u1ec9a \u0111\u01b0\u1ee3c l\u1eb7p l\u1ea1i \u0111\u1ebfn khi \\(\\mathcal{L}\\) kh\u00f4ng th\u1ec3 gi\u1ea3m \u0111\u01b0\u1ee3c n\u1eefa.</p> </li> </ol>"},{"location":"course/ml-algorithms/decision-tree/#code","title":"Code","text":"<p>Download d\u1eef li\u1ec7u <code>weather.csv</code> t\u1ea1i \u0111\u00e2y.</p> <p>\u0110\u1ea7u ti\u00ean ta import c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft:</p> <pre><code>import pandas as pd\nimport numpy as np\n</code></pre> <p>Load d\u1eef li\u1ec7u:</p> <pre><code>df = pd.read_csv(\"weather.csv\")\nprint(df)\n</code></pre> <pre><code>    id   outlook temperature humidity    wind play\n0    1     sunny         hot     high    weak   no\n1    2     sunny         hot     high  strong   no\n2    3  overcast         hot     high    weak  yes\n3    4     rainy        mild     high    weak  yes\n4    5     rainy        cool   normal    weak  yes\n5    6     rainy        cool   normal  strong   no\n6    7  overcast        cool   normal  strong  yes\n7    8     sunny        mild     high    weak   no\n8    9     sunny        cool   normal    weak  yes\n9   10     rainy        mild   normal    weak  yes\n10  11     sunny        mild   normal  strong  yes\n11  12  overcast        mild     high  strong  yes\n12  13  overcast         hot   normal    weak  yes\n13  14     rainy        mild     high  strong   no\n</code></pre> <p>Vi\u1ebft h\u00e0m t\u00ednh entropy c\u1ee7a m\u1ed9t b\u1ea3ng d\u1eef li\u1ec7u: (1)</p> <ol> <li>Nh\u1eafc l\u1ea1i: $$ \\tag{2}     H(\\mathcal{S}) = - \\sum_{c \\in \\mathbf{C}} \\dfrac{N_c}{N} \\log_2 \\left( \\dfrac{N_c}{N} \\right)     $$</li> </ol> <pre><code>def entropy(data: pd.DataFrame):\n    y = data.iloc[:, -1]  # Retrieve the labels\n    value_counts = y.value_counts()  # Calculate all N_c\n    probs = value_counts / len(y)  # Calculate all N_c / N\n    entropy = -np.sum(probs * np.log2(probs))  # Calculate the entropy\n    return entropy\n</code></pre> <p>Vi\u1ebft h\u00e0m t\u00ednh weighted entropy c\u1ee7a m\u1ed9t b\u1ea3ng d\u1eef li\u1ec7u v\u00e0 m\u1ed9t thu\u1ed9c t\u00ednh: (1)</p> <ol> <li>Nh\u1eafc l\u1ea1i: $$ \\tag{3}     H(x, \\mathcal{S}) = \\sum_{k \\in K} \\frac{m_k}{N} H(\\mathcal{S}_k)     $$</li> </ol> <pre><code>def weighted_entropy(data: pd.DataFrame, prop: str):\n    N = len(data)\n    weighted_entropy = 0\n\n    for value in data[prop].unique():\n        subset = data[data[prop] == value]  # Calculate m_k\n        value_entropy = entropy(subset)  # Calculate H(S_k)\n\n        weighted_entropy += value_entropy * len(subset) / N  # Add to the final result\n\n    return weighted_entropy\n</code></pre> <p>Ta c\u00f3 th\u1ec3 th\u1eed ch\u1ea1y h\u00e0m <code>weighted_entropy</code> tr\u00ean b\u1ea3ng d\u1eef li\u1ec7u ban \u0111\u1ea7u xem k\u1ebft qu\u1ea3 \u0111\u00e3 \u0111\u00fang ch\u01b0a:</p> <p><pre><code>for col in [\"outlook\", \"temperature\", \"humidity\", \"wind\"]:\n    print(col, weighted_entropy(df, col))\n</code></pre> <pre><code>outlook 0.6935361388961919\ntemperature 0.9110633930116763\nhumidity 0.7884504573082896\nwind 0.8921589282623617\n</code></pre> Ta th\u1ea5y n\u00f3 kh\u1edbp v\u1edbi k\u1ebft qu\u1ea3 ta \u0111\u00e3 t\u00ednh ph\u00eda tr\u00ean. Nh\u01b0 v\u1eady c\u00f3 v\u1ebb h\u00e0m <code>entropy</code> v\u00e0 <code>weighted_entropy</code> \u0111\u00e3 \u1ed5n.</p> <p>Gi\u1edd ta s\u1ebd tri\u1ec3n khai decision tree. \u0110\u1ea7u ti\u00ean, ta vi\u1ebft class <code>TreeNode</code> nh\u01b0 sau: <pre><code>class TreeNode:\n    def __init__(self, row_indices: List[int]) -&gt; None:\n        self.row_indices = row_indices  # Indices of data points of the node \n        self.split_column = None  # Property to split the data\n        self.children = {}  # A dictionary of &lt;column value&gt; : &lt;child node&gt;\n        self.label = None\n\n    def add(self, child_node, column_value):\n        self.children[column_value] = child_node\n\n    def __str__(self) -&gt; str:\n        return str(self.row_indices)\n</code></pre> V\u00e0 class <code>DecisionTree</code>: <pre><code>class DecisionTree:\n    def __init__(self) -&gt; None:\n        self.root = None  # to store the root TreeNode\n        self.leaves = []  # to store leaf nodes\n</code></pre></p> <p>V\u1edbi h\u00e0m training cho Decision Tree, ta s\u1eed d\u1ee5ng thu\u1eadt to\u00e1n BFS \u0111\u1ec3 duy\u1ec7t c\u00e2y (code h\u01a1i d\u00e0i nh\u01b0ng c\u00f3 coment \u0111\u1ea7y \u0111\u1ee7) <pre><code>    def fit(self, data):\n        # Initialize the root node with all row indices from the dataset\n        self.root = TreeNode(list(range(len(data))))\n\n        # Exclude the label column from the features\n        X = data.iloc[:, :-1]\n\n        # Initialize the queue for BFS\n        queue = deque([self.root])\n\n        # Perform BFS to build the decision tree\n        while queue:\n            # Retrieve the current node from the queue\n            node = queue.popleft()\n\n            # Get the subset of data corresponding to the current node\n            node_data = data.iloc[node.row_indices]\n\n            # Check if the current node is a leaf node\n            if entropy(node_data) &lt; 1e-6:\n                node.label = node_data.iloc[:, -1].mode()[0]  # Assign the most frequent label to the leaf node\n                self.leaves.append(node)  # Add the leaf node to the list of leaves\n                continue  # Stop further splitting for this node\n\n            # Find the feature with the minimum entropy for splitting\n            node.split_column = min(X.columns, key=lambda col: weighted_entropy(node_data, col))\n\n            # Group the data indices by the selected split column.\n            # Note: 'splits' contains indices relative to 'node_data', not the original data.\n            splits = node_data.groupby(node_data[node.split_column]).indices\n\n            # Convert grouped indices back to the original data indices\n            for val in splits:\n                splits[val] = [node.row_indices[ind] for ind in splits[val]]\n\n            # Create child nodes for each group and add them to the current node\n            for value in splits:\n                indices = splits[value]\n                child_node = TreeNode(indices)\n                node.add(child_node, value)\n                queue.append(child_node)  # Add the child node to the queue for further processing\n</code></pre> Cu\u1ed1i c\u00f9ng l\u00e0 h\u00e0m predict. V\u1edbi m\u1ed9t b\u1ea3ng d\u1eef li\u1ec7u m\u1edbi, ta s\u1ebd duy\u1ec7t t\u1eebng h\u00e0ng c\u1ee7a b\u1ea3ng d\u1eef li\u1ec7u, \u0111\u01b0a v\u00e0o decision tree v\u1eeba x\u00e2y d\u1ef1ng \u0111\u1ec3 predict cho t\u1eebng h\u00e0ng. <pre><code>    def predict(self, new_data):\n        npoints = new_data.count()[0]  # Retrieve the number of data points in new_data\n        labels = [None] * npoints  # Initialize the list to store prediction results\n\n        for n in range(npoints):\n            x = new_data.iloc[n, :]  # Get the current data point\n            node = self.root  # Start prediction from the root node\n\n            # Traverse the tree until a leaf node is reached\n            while node.children:\n                col_to_split = node.split_column  # Get the column used for splitting at this node\n                if x[col_to_split] not in node.children:  # Check if the split value exists in the children of the current node\n                    break  # If the value is not found, this data point cannot be classified\n\n                # Move to the corresponding child node based on the split value\n                node = node.children[x[col_to_split]]\n\n            # Assign the label of the reached leaf node to the current data point\n            labels[n] = node.label\n\n        return labels\n</code></pre> Nh\u01b0 v\u1eady l\u00e0 ta \u0111\u00e3 code xong decision tree. Gi\u1edd ta s\u1ebd ch\u1ea1y th\u1eed tr\u00ean ch\u00ednh t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 cho \u0111\u1ec3 xem t\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c ph\u00e2n lo\u1ea1i \u0111\u00fang hay ch\u01b0a. <pre><code>if __name__ == \"__main__\":\n    df = pd.read_csv(\"weather.csv\")\n    X = df.iloc[:, 1:]\n    y = df.iloc[:, -1]\n\n    dt = DecisionTree()\n    dt.fit(X)\n\n    print(dt.predict(X))\n    print(y.to_list())\n</code></pre> <pre><code>['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']\n['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']\n</code></pre> K\u1ebft qu\u1ea3 ph\u00e2n lo\u1ea1i c\u1ee7a decision tree tr\u00f9ng kh\u1edbp ho\u00e0n to\u00e0n v\u1edbi label ban \u0111\u1ea7u c\u1ee7a data, nh\u01b0 v\u1eady l\u00e0 ta \u0111\u00e3 c\u00e0i \u0111\u1eb7t th\u00e0nh c\u00f4ng decision tree.</p> <p>Ta c\u00f3 th\u1ec3 v\u1ebd l\u1ea1i decision tree ta v\u1eeba x\u00e2y d\u1ef1ng nh\u01b0 sau: <pre><code>flowchart TD\n    root(\n        row_indices = 0, 1, ..., 13\n        split_column = outlook\n    )\n\n    child1(\n        row_indices = 0, 1, 7, 8, 10\n        split_column = humidity\n    )\n\n    child2(\n        row_indices = 3, 4, 5, 9, 13\n        split_column = wind\n    )\n\n    leaf1((N))\n    leaf2((Y))\n    leaf3((Y))\n    leaf4((Y))\n    leaf5((N))\n\n    root -- sunny ---&gt; child1\n    root -- overcast --&gt; leaf3\n    root -- rainy ---&gt; child2\n\n    child1 -- high --&gt; leaf1\n    child1 -- normal --&gt; leaf2\n\n    child2 -- weak --&gt; leaf4\n    child2 -- strong --&gt; leaf5\n\n    style leaf1 fill:#f3b7b6,stroke:#555,stroke-width:1px,color:#000\n    style leaf2 fill:#c4fdbb,stroke:#555,stroke-width:1px,color:#000\n    style leaf3 fill:#c4fdbb,stroke:#555,stroke-width:1px,color:#000\n    style leaf4 fill:#c4fdbb,stroke:#555,stroke-width:1px,color:#000\n    style leaf5 fill:#f3b7b6,stroke:#555,stroke-width:1px,color:#000</code></pre></p> Code ho\u00e0n ch\u1ec9nh <code>decision_tree.py</code> decision_tree.py<pre><code>from typing import List\nimport pandas as pd\nimport numpy as np\nfrom collections import deque\n\n\ndef entropy(data: pd.DataFrame):\n    y = data.iloc[:, -1]  # Retrieve the labels\n    value_counts = y.value_counts()  # Calculate all N_c\n    probs = value_counts / len(y)  # Calculate all N_c / N\n    entropy = -np.sum(probs * np.log2(probs))  # Calculate the entropy\n    return entropy\n\n\ndef weighted_entropy(data: pd.DataFrame, prop: str):\n    N = len(data)\n    weighted_entropy = 0\n\n    for value in data[prop].unique():\n        subset = data[data[prop] == value]  # Calculate m_k\n        value_entropy = entropy(subset)  # Calculate H(S_k)\n\n        weighted_entropy += value_entropy * len(subset) / N  # Add to the final result\n\n    return weighted_entropy\n\n\nclass TreeNode:\n    def __init__(self, row_indices) -&gt; None:\n        self.row_indices = row_indices\n        self.split_column = None\n        self.children = {}\n        self.label = None\n\n    def add(self, child_node, column_value):\n        self.children[column_value] = child_node\n\n    def __str__(self) -&gt; str:\n        return str(self.row_indices)\n\n\nclass DecisionTree:\n    def __init__(self) -&gt; None:\n        self.root = None  # The root node of the decision tree\n        self.leaves = []  # List to store all the leaf nodes\n\n    def fit(self, data):\n        # Initialize the root node with all row indices from the dataset\n        self.root = TreeNode(list(range(len(data))))\n\n        # Exclude the label column from the features\n        X = data.iloc[:, :-1]\n\n        # Initialize the queue for BFS\n        queue = deque([self.root])\n\n        # Perform BFS to build the decision tree\n        while queue:\n            # Retrieve the current node from the queue\n            node = queue.popleft()\n\n            # Get the subset of data corresponding to the current node\n            node_data = data.iloc[node.row_indices]\n\n            # Check if the current node is a leaf node\n            if entropy(node_data) &lt; 1e-6:\n                node.label = node_data.iloc[:, -1].mode()[0]  # Assign the most frequent label to the leaf node\n                self.leaves.append(node)  # Add the leaf node to the list of leaves\n                continue  # Stop further splitting for this node\n\n            # Find the feature with the minimum entropy for splitting\n            node.split_column = min(X.columns, key=lambda col: weighted_entropy(node_data, col))\n\n            # Group the data indices by the selected split column.\n            # Note: 'splits' contains indices relative to 'node_data', not the original data.\n            splits = node_data.groupby(node_data[node.split_column]).indices\n\n            # Convert grouped indices back to the original data indices\n            for val in splits:\n                splits[val] = [node.row_indices[ind] for ind in splits[val]]\n\n            # Create child nodes for each group and add them to the current node\n            for value in splits:\n                indices = splits[value]\n                child_node = TreeNode(indices)\n                node.add(child_node, value)\n                queue.append(child_node)  # Add the child node to the queue for further processing\n\n    def predict(self, new_data):\n        npoints = new_data.count()[0]  # Retrieve the number of data points in new_data\n        labels = [None] * npoints  # Initialize the list to store prediction results\n\n        for n in range(npoints):\n            x = new_data.iloc[n, :]  # Get the current data point\n            node = self.root  # Start prediction from the root node\n\n            # Traverse the tree until a leaf node is reached\n            while node.children:\n                col_to_split = node.split_column  # Get the column used for splitting at this node\n                if x[col_to_split] not in node.children:  # Check if the split value exists in the children of the current node\n                    break  # If the value is not found, this data point cannot be classified\n\n                # Move to the corresponding child node based on the split value\n                node = node.children[x[col_to_split]]\n\n            # Assign the label of the reached leaf node to the current data point\n            labels[n] = node.label\n\n        return labels\n\n\nif __name__ == \"__main__\":\n    df = pd.read_csv(\"weather.csv\")\n    X = df.iloc[:, 1:]\n    y = df.iloc[:, -1]\n\n    dt = DecisionTree()\n    dt.fit(X)\n\n    print(dt.predict(X))\n    print(y.to_list())\n</code></pre> <ol> <li> <p>https://machinelearningcoban.com/2018/01/14/id3/\u00a0\u21a9</p> </li> </ol>"},{"location":"leetcode/","title":"LeetCode Series","text":"<p>\u0110\u00e2y l\u00e0 t\u1ed5ng h\u1ee3p c\u00e1c b\u00e0i t\u1eadp tr\u00ean  LeetCode ch\u1ecdn l\u1ecdc c\u00f9ng c\u00e1ch gi\u1ea3i v\u00e0 code.</p> <p>V\u00ec c\u00e1c b\u00e0i t\u1eadp tr\u00ean  LeetCode th\u01b0\u1eddng kh\u00f4ng c\u00f3 format cho Input v\u00e0 Output n\u00ean c\u00f3 th\u1ec3 \u1edf d\u01b0\u1edbi s\u1ebd t\u1ef1 b\u1ecba.</p> <p>T\u00ean c\u00e1c bi\u1ebfn xu\u1ea5t hi\u1ec7n trong \u0111\u1ec1 b\u00e0i n\u1ebfu c\u1ea7n c\u0169ng s\u1ebd \u0111\u1ed5i lu\u00f4n cho n\u00f3 d\u1ec5 nh\u00ecn v\u1ec1 m\u1eb7t to\u00e1n h\u1ecdc.</p> <p>Gi\u1edbi h\u1ea1n r\u00e0ng bu\u1ed9c c\u1ee7a b\u00e0i to\u00e1n c\u00f3 th\u1ec3 c\u0169ng thay lu\u00f4n, v\u00ec nhi\u1ec1u khi  LeetCode \u0111\u1ec3 r\u00e0ng bu\u1ed9c qu\u00e1 l\u1ecfng.</p> <p>Ch\u1ee7 y\u1ebfu l\u00e0 \u0111\u1ec3 t\u00e1i s\u1eed d\u1ee5ng n\u1ebfu c\u1ea7n. C\u00e1c b\u00e0i to\u00e1n h\u1ea7u h\u1ebft s\u1ebd \u0111\u01b0\u1ee3c code b\u1eb1ng </p> <p>L\u01b0u \u00fd</p> <p>N\u1ebfu c\u00e1c c\u00f4ng th\u1ee9c to\u00e1n trong b\u00e0i vi\u1ebft kh\u00f4ng hi\u1ec3n th\u1ecb \u0111\u00fang, b\u1ea5m F5 \u0111\u1ec3 t\u1ea3i l\u1ea1i trang l\u1ea7n n\u1eefa.</p>"},{"location":"leetcode/#danh-sach-cac-bai-tap-leetcode","title":"Danh s\u00e1ch c\u00e1c b\u00e0i t\u1eadp LeetCode","text":""},{"location":"leetcode/#0-999","title":"0 - 999","text":"<ul> <li>714.\u00a0Best Time to Buy and Sell Stock with Transaction Fee</li> </ul>"},{"location":"leetcode/#1000-1999","title":"1000 - 1999","text":"<ul> <li>1175.\u00a0Prime Arrangements</li> </ul>"},{"location":"leetcode/#2000-2999","title":"2000 - 2999","text":"<ul> <li>2485.\u00a0Find the Pivot Integer</li> </ul>"},{"location":"leetcode/1175/","title":"1175.\u00a0Prime Arrangements","text":"<p>Easy</p> <p>Cho s\u1ed1 t\u1ef1 nhi\u00ean \\(n\\), tr\u1ea3 v\u1ec1 s\u1ed1 ho\u00e1n v\u1ecb c\u1ee7a c\u00e1c s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(n\\) sao cho c\u00e1c s\u1ed1 nguy\u00ean t\u1ed1 \u0111\u1ee9ng \u1edf c\u00e1c v\u1ecb tr\u00ed nguy\u00ean t\u1ed1 (v\u1ecb tr\u00ed \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 b\u1eaft \u0111\u1ea7u t\u1eeb \\(1\\)).</p> <p>V\u00ec k\u1ebft qu\u1ea3 c\u00f3 th\u1ec3 r\u1ea5t l\u1edbn n\u00ean tr\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 sau khi chia l\u1ea5y d\u01b0 cho \\(10^9+7\\).</p> <p>Input</p> <ul> <li>G\u1ed3m m\u1ed9t d\u00f2ng duy nh\u1ea5t ch\u1ee9a \\(n\\)</li> </ul> <p>Output</p> <ul> <li>G\u1ed3m m\u1ed9t d\u00f2ng duy nh\u1ea5t l\u00e0 k\u1ebft qu\u1ea3 c\u1ee7a b\u00e0i to\u00e1n</li> </ul> <p>R\u00e0ng bu\u1ed9c</p> <ul> <li>\\(1 \\leq n \\leq 100\\)</li> </ul> <p>V\u00ed d\u1ee5</p> <p>V\u00ed d\u1ee5 1</p> <p>Input<pre><code>5\n</code></pre> Output<pre><code>12\n</code></pre> Gi\u1ea3i th\u00edch: Ch\u1eb3ng h\u1ea1n ho\u00e1n v\u1ecb \\([1, 2, 5, 4, 3]\\) l\u00e0 m\u1ed9t ho\u00e1n v\u1ecb tho\u1ea3 m\u00e3n, nh\u01b0ng ho\u00e1n v\u1ecb \\([5, 2, 3, 4, 1]\\) th\u00ec kh\u00f4ng.</p> <p>V\u00ed d\u1ee5 2</p> <p>Input<pre><code>100\n</code></pre> Output<pre><code>682289015\n</code></pre></p> Link n\u1ed9p b\u00e0i <p>1175.\u00a0Prime Arrangements</p> Solution <p>Gi\u1ea3 s\u1eed c\u00f3 \\(p\\) s\u1ed1 nguy\u00ean t\u1ed1 trong \u0111o\u1ea1n t\u1eeb \\(1\\) \u0111\u1ebfn \\(n\\). V\u00ec c\u00e1c s\u1ed1 nguy\u00ean t\u1ed1 ch\u1ec9 c\u00f3 th\u1ec3 \u1edf v\u1ecb tr\u00ed nguy\u00ean t\u1ed1 n\u00ean ta c\u00f3 \\(p!\\) c\u00e1ch ch\u1ecdn v\u1ecb tr\u00ed cho \\(p\\) s\u1ed1 n\u00e0y.</p> <p>V\u1edbi m\u1ed7i c\u00e1ch ch\u1ecdn v\u1ecb tr\u00ed cho \\(p\\) s\u1ed1 nguy\u00ean t\u1ed1, ta c\u00f3 \\((n - p)!\\) c\u00e1ch ch\u1ecdn v\u1ecb tr\u00ed cho c\u00e1c s\u1ed1 c\u00f2n l\u1ea1i.</p> <p>Nh\u01b0 v\u1eady k\u1ebft qu\u1ea3 b\u00e0i to\u00e1n l\u00e0:</p> \\[ p!(n-p)! \\] <p>v\u1edbi \\(p\\) l\u00e0 s\u1ed1 s\u1ed1 nguy\u00ean t\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(n\\).</p> <p>Nh\u01b0 v\u1eady ch\u1ec9 c\u1ea7n t\u00ecm \\(p\\) l\u00e0 s\u1ed1 l\u01b0\u1ee3ng s\u1ed1 nguy\u00ean t\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(n\\) l\u00e0 ta c\u00f3 th\u1ec3 gi\u1ea3i b\u00e0i to\u00e1n n\u00e0y.</p> <p>\u0110\u1ec1 b\u00e0i ch\u1ec9 cho gi\u1edbi h\u1ea1n \\(n \\leq 100\\) n\u00ean th\u00edch qu\u1ea9y s\u00e0ng nguy\u00ean t\u1ed1 hay c\u00e1i g\u00ec c\u0169ng \u0111\u01b0\u1ee3c.</p> Code  C++ Python <pre><code>class Solution {\npublic:\n    const long long MOD = 1000000007;\n\n    long long numPrimeArrangements(long long n) {\n\n        // S\u00e0ng nguy\u00ean t\u1ed1\n        vector&lt;bool&gt; isPrime(n + 1, true);\n        isPrime[0] = false;\n        isPrime[1] = false;\n\n        for (int i = 2; i * i &lt;= n; i++) {\n            if (isPrime[i])\n                for (int j = i * i; j &lt;= n; j += i) {\n                    isPrime[j] = false;\n                }\n        }\n\n        // \u0110\u1ebfm s\u1ed1 s\u1ed1 nguy\u00ean t\u1ed1 trong kho\u1ea3ng t\u1eeb 1 \u0111\u1ebfn n\n        long long count = 0;\n        for (long long i = 0; i &lt;= n; i++) {\n            if (isPrime[i])\n                count++;\n        }\n\n        // T\u00ednh k\u1ebft qu\u1ea3\n        long long result = 1;\n        for (long long i = 1; i &lt;= count; i++) {\n            result = (result * i) % MOD;\n        }\n        for (long long i = 1; i &lt;= n - count; i++) {\n            result = (result * i) % MOD;\n        }\n\n        return result;\n    }\n};\n</code></pre> <pre><code>class Solution:\n    def numPrimeArrangements(self, n: int) -&gt; int:\n        MOD = int(1e9 + 7)\n\n        is_prime = [True] * (n + 1)\n        is_prime[0] = False\n        is_prime[1] = False\n\n        for i in range(2, int(n ** .5) + 1):\n            for j in range(i * i, n + 1, i):\n                is_prime[j] = False\n\n        count = 0\n        for i in range(n + 1):\n            if is_prime[i]:\n                count += 1\n\n        ans = 1\n        for i in range(1, count + 1):\n            ans = (ans * i) % MOD\n        for i in range(1, n - count + 1):\n            ans = (ans * i) % MOD\n\n        return ans\n</code></pre>"},{"location":"leetcode/2485/","title":"2485.\u00a0Find the Pivot Integer","text":"<p>Easy</p> <p>Cho m\u1ed9t s\u1ed1 t\u1ef1 nhi\u00ean \\(n\\). H\u00e3y t\u00ecm s\u1ed1 t\u1ef1 nhi\u00ean \\(x\\) sao cho t\u1ed5ng c\u1ee7a c\u00e1c s\u1ed1 t\u1eeb \\(1\\) \u0111\u1ebfn \\(x\\) b\u1eb1ng t\u1ed5ng c\u1ee7a c\u00e1c s\u1ed1 t\u1eeb \\(x\\) \u0111\u1ebfn \\(n\\). N\u1ebfu kh\u00f4ng c\u00f3 s\u1ed1 t\u1ef1 nhi\u00ean n\u00e0o tho\u1ea3 m\u00e3n th\u00ec in ra \\(-1\\).</p> <p>Input</p> <ul> <li>G\u1ed3m m\u1ed9t d\u00f2ng duy nh\u1ea5t ch\u1ee9a \\(n\\)</li> </ul> <p>Output</p> <ul> <li>G\u1ed3m m\u1ed9t d\u00f2ng duy nh\u1ea5t l\u00e0 k\u1ebft qu\u1ea3 c\u1ee7a b\u00e0i to\u00e1n</li> </ul> <p>R\u00e0ng bu\u1ed9c</p> <ul> <li>\\(1 \\leq n \\leq 10^3\\)</li> </ul> <p>V\u00ed d\u1ee5</p> <p>V\u00ed d\u1ee5 1</p> <p>Input<pre><code>8\n</code></pre> Output<pre><code>6\n</code></pre> Gi\u1ea3i th\u00edch: \\(1 + 2 + 3 + 4 + 5 + 6 = 6 + 7 + 8 = 21\\).</p> <p>V\u00ed d\u1ee5 2</p> <p>Input<pre><code>1\n</code></pre> Output<pre><code>1\n</code></pre></p> <p>V\u00ed d\u1ee5 3</p> <p>Input<pre><code>4\n</code></pre> Output<pre><code>-1\n</code></pre></p> Link n\u1ed9p b\u00e0i <p>2485.\u00a0Find the Pivot Integer</p> Solution <p>To\u00e1n th\u00f4ng th\u01b0\u1eddng:</p> \\[ \\begin{align*}     &amp;&amp;1 + 2 + \\dots + x &amp;= x + (x + 1) + \\dots + n \\\\  \\Leftrightarrow &amp;&amp;\\dfrac{x(x+ 1)}{2} &amp;= \\dfrac{n(n+ 1)}{2} - \\dfrac{(x - 1)x}{2} \\\\  \\Leftrightarrow &amp;&amp; 2x^2 &amp;= n(n+1) \\\\  \\Leftrightarrow &amp;&amp; x &amp;= \\sqrt{\\dfrac{n(n + 1)}{2}}, \\quad \\text{do } x &gt; 0 \\end{align*} \\] <p>Nh\u01b0 v\u1eady ta ch\u1ec9 c\u1ea7n ki\u1ec3m tra \\(\\dfrac{n(n + 1)}{2}\\) c\u00f3 ph\u1ea3i s\u1ed1 ch\u00ednh ph\u01b0\u01a1ng kh\u00f4ng l\u00e0 \u0111\u01b0\u1ee3c. N\u1ebfu c\u00f3 th\u00ec tr\u1ea3 v\u1ec1 \\(x\\) nh\u01b0 gi\u1ea3i \u1edf tr\u00ean, n\u1ebfu kh\u00f4ng th\u00ec tr\u1ea3 v\u1ec1 <code>-1</code>.</p> <p>\u0110\u1ed9 ph\u1ee9c t\u1ea1p l\u00e0 \\(\\text{O}(1)\\). B\u00e0i n\u00e0y \u0111\u00e1ng l\u1ebd ph\u1ea3i cho gi\u1edbi h\u1ea1n \\(n\\) l\u00e0 \\(10^9\\).</p> Code  C++ Python <pre><code>class Solution {\npublic:\n    int pivotInteger(int n) {\n        int x = (int) sqrt((n * n + n) / 2);\n        return 2 * x * x == n * (n + 1) ? x : -1;\n    }\n};\n</code></pre> <pre><code>class Solution:\ndef pivotInteger(self, n: int) -&gt; int:\n    ans = int((n * (n + 1) // 2) ** .5)\n    if ans * ans == n * (n + 1) // 2:\n        return ans\n    else:\n        return -1\n</code></pre>"},{"location":"leetcode/714/","title":"714.\u00a0Best Time to Buy and Sell Stock with Transaction Fee","text":"<p>Medium</p> <p>Cho m\u1ea3ng <code>prices</code> g\u1ed3m <code>n</code> ph\u1ea7n t\u1eed, trong \u0111\u00f3 <code>prices[i]</code> l\u00e0 gi\u00e1 c\u1ee7a m\u1ed9t lo\u1ea1i c\u1ed5 phi\u1ebfu v\u00e0o ng\u00e0y th\u1ee9 <code>i</code>. Ph\u00ed giao d\u1ecbch l\u00e0 <code>fee</code>.</p> <p>H\u00e3y t\u00ecm l\u1ee3i nhu\u1eadn t\u1ed1i \u0111a c\u00f3 th\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c nh\u1edd vi\u1ec7c mua b\u00e1n c\u1ed5 phi\u1ebfu. B\u1ea1n c\u00f3 th\u1ec3 giao d\u1ecbch bao nhi\u00eau l\u1ea7n b\u1ea1n mu\u1ed1n, nh\u01b0ng b\u1ea1n c\u1ea7n ph\u1ea3i tr\u1ea3 <code>fee</code> \u0111\u1ed3ng cho m\u1ed7i giao d\u1ecbch.</p> <p>L\u01b0u \u00fd: B\u1ea1n kh\u00f4ng \u0111\u01b0\u1ee3c ph\u00e9p tham gia nhi\u1ec1u giao d\u1ecbch c\u00f9ng m\u1ed9t l\u00fac, ngh\u0129a l\u00e0 b\u1ea1n ph\u1ea3i b\u00e1n c\u1ed5 phi\u1ebfu th\u00ec m\u1edbi \u0111\u01b0\u1ee3c ti\u1ebfp t\u1ee5c mua l\u1ea1i.</p> <p>Input</p> <ul> <li>D\u00f2ng \u0111\u1ea7u ti\u00ean ch\u1ee9a 2 s\u1ed1 nguy\u00ean <code>n</code> v\u00e0 <code>fee</code></li> <li>D\u00f2ng th\u1ee9 hai ch\u1ee9a <code>n</code> s\u1ed1 nguy\u00ean c\u1ee7a <code>prices</code></li> </ul> <p>Output</p> <ul> <li>G\u1ed3m m\u1ed9t d\u00f2ng duy nh\u1ea5t l\u00e0 k\u1ebft qu\u1ea3 c\u1ee7a b\u00e0i to\u00e1n</li> </ul> <p>R\u00e0ng bu\u1ed9c</p> <ul> <li>\\(1 \\leq\\) <code>n</code> \\(\\leq 5 \\times 10^4\\)</li> <li>\\(1 \\leq\\) <code>prices[i]</code> \\(&lt; 5 \\times 10^4\\)</li> <li>\\(0 \\leq\\) <code>fee</code> \\(&lt; 5 \\times 10^4\\)</li> </ul> <p>V\u00ed d\u1ee5</p> <p>V\u00ed d\u1ee5 1</p> <p>Input<pre><code>6 2\n1 3 2 8 4 9\n</code></pre> Output<pre><code>8\n</code></pre> Gi\u1ea3i th\u00edch: L\u1ee3i nhu\u1eadn t\u1ed1i \u0111a \u0111\u1ea1t \u0111\u01b0\u1ee3c b\u1eb1ng c\u00e1ch:</p> <ul> <li>Mua \u1edf ng\u00e0y 0, gi\u00e1 b\u1eb1ng \\(1\\)</li> <li>B\u00e1n \u1edf ng\u00e0y 3, gi\u00e1 b\u1eb1ng \\(8\\)</li> <li>Mua \u1edf ng\u00e0y 4, gi\u00e1 b\u1eb1ng \\(4\\)</li> <li>B\u00e1n \u1edf ng\u00e0y 5, gi\u00e1 b\u1eb1ng \\(9\\)</li> </ul> <p>T\u1ed5ng l\u1ee3i nhu\u1eadn l\u00e0 \\(((8 - 1) - 2) + ((9 - 4) - 2) = 8\\).</p> <p>V\u00ed d\u1ee5 2</p> <p>Input<pre><code>6 3\n1 3 7 5 10 3\n</code></pre> Output<pre><code>6\n</code></pre></p> Link n\u1ed9p b\u00e0i <p>714.\u00a0Best Time to Buy and Sell Stock with Transaction Fee</p> Solution <p>Quy ho\u1ea1ch \u0111\u1ed9ng: G\u1ecdi <code>free[i]</code> l\u00e0 l\u1ee3i nhu\u1eadn l\u1edbn nh\u1ea5t n\u1ebfu kh\u00f4ng gi\u1eef c\u1ed5 phi\u1ebfu v\u00e0o ng\u00e0y th\u1ee9 <code>i</code> v\u00e0 <code>hold[i]</code> l\u00e0 l\u1ee3i nhu\u1eadn l\u1edbn nh\u1ea5t n\u1ebfu gi\u1eef c\u1ed5 phi\u1ebfu v\u00e0o ng\u00e0y th\u1ee9 <code>i</code>.</p> <p>C\u00f4ng th\u1ee9c quy ho\u1ea1ch \u0111\u1ed9ng:</p> <ul> <li><code>free[i] = max(free[i - 1], hold[i - 1] + prices[i] - fee)</code> (Ho\u1eb7c tr\u01b0\u1edbc \u0111\u00f3 \u0111ang kh\u00f4ng gi\u1eef c\u1ed5 phi\u1ebfu v\u00e0 sau \u0111\u00f3 kh\u00f4ng l\u00e0m g\u00ec v\u00e0o ng\u00e0y th\u1ee9 <code>i</code>, ho\u1eb7c b\u00e1n c\u1ed5 phi\u1ebfu \u0111\u00f3 v\u00e0o ng\u00e0y th\u1ee9 <code>i</code>);</li> <li><code>hold[i] = max(hold[i - 1], free[i - 1] - prices[i]</code> (Ho\u1eb7c tr\u01b0\u1edbc \u0111\u00f3 \u0111ang gi\u1eef c\u1ed5 phi\u1ebfu v\u00e0 sau \u0111\u00f3 kh\u00f4ng l\u00e0m g\u00ec v\u00e0o ng\u00e0y th\u1ee9 <code>i</code>, ho\u1eb7c tr\u01b0\u1edbc \u0111\u00f3 \u0111ang kh\u00f4ng gi\u1eef c\u1ed5 phi\u1ebfu v\u00e0 mua c\u1ed5 phi\u1ebfu v\u00e0o ng\u00e0y th\u1ee9 <code>i</code>.</li> </ul> <p>K\u1ebft qu\u1ea3 b\u00e0i to\u00e1n l\u00e0 <code>free[n - 1]</code> (c\u00e1c ch\u1ec9 s\u1ed1 \u0111\u01b0\u1ee3c \u0111\u00e1nh s\u1ed1 b\u1eaft \u0111\u1ea7u t\u1eeb \\(0\\)).</p> <p>V\u00ec <code>free[i]</code> v\u00e0 <code>hold[i]</code> ch\u1ec9 ph\u1ee5 thu\u1ed9c v\u00e0o <code>free[i - 1]</code> v\u00e0 <code>hold[i - 1]</code>, n\u00ean thay v\u00ec d\u00f9ng m\u1ea3ng \u0111\u1ec3 l\u01b0u th\u00ec ta ch\u1ec9 c\u1ea7n m\u1ed9t bi\u1ebfn <code>free</code> \u0111\u1ec3 l\u01b0u l\u1ee3i nhu\u1eadn l\u1edbn nh\u1ea5t n\u1ebfu kh\u00f4ng gi\u1eef c\u1ed5 phi\u1ebfu v\u00e0o ng\u00e0y th\u1ee9 <code>i</code> v\u00e0 m\u1ed9t bi\u1ebfn <code>hold</code> \u0111\u1ec3 l\u01b0u l\u1ee3i nhu\u1eadn l\u1edbn nh\u1ea5t n\u1ebfu gi\u1eef c\u1ed5 phi\u1ebfu v\u00e0o ng\u00e0y th\u1ee9 <code>i</code>. L\u00fac duy\u1ec7t ta s\u1ebd c\u1eadp nh\u1eadt 2 bi\u1ebfn n\u00e0y.</p> <p>\u0110\u1ed9 ph\u1ee9c t\u1ea1p \\(\\text{O}(n)\\).</p> Code  C++ Python <pre><code>class Solution {\npublic:\n    int maxProfit(vector&lt;int&gt;&amp; prices, int fee) {\n        int n = prices.size();\n        int free = 0, hold = -prices[0];\n\n        for (int i = 1; i &lt; n; i++) {\n            int tmp = hold;\n            hold = max(hold, free - prices[i]);\n            free = max(free, tmp + prices[i] - fee);\n        }\n\n        return free;\n    }\n};\n</code></pre> <pre><code>class Solution:\ndef maxProfit(self, prices: List[int], fee: int) -&gt; int:\n    free = 0\n    hold = -prices[0]\n\n    for i in range(1, len(prices)):\n        hold, free = max(hold, free - prices[i]), max(free, hold + prices[i] - fee)\n\n    return free\n</code></pre>"},{"location":"blog/category/flag_gb/","title":"\ud83c\uddec\ud83c\udde7","text":""},{"location":"blog/category/system-design/","title":"System Design","text":""},{"location":"blog/category/flag_vn/","title":"\ud83c\uddfb\ud83c\uddf3","text":""},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#cap-theorem","title":"CAP theorem","text":"<ul> <li>The CAP theorem</li> </ul>"},{"location":"tags/#hadoop","title":"Hadoop","text":"<ul> <li>Gi\u1edbi thi\u1ec7u v\u1ec1 Apache Hadoop</li> </ul>"},{"location":"tags/#load-balancing","title":"Load Balancing","text":"<ul> <li>Load Balancing Algorithms</li> </ul>"}]}